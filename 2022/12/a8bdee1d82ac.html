<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ai | Sinos_wei's blog</title><meta name="author" content="Sinos"><meta name="copyright" content="Sinos"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="article">
<meta property="og:title" content="ai">
<meta property="og:url" content="https://sinos_wei.gitee.io/2022/12/a8bdee1d82ac.html">
<meta property="og:site_name" content="Sinos_wei&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://sinos_wei.gitee.io/img/cover/39.jpeg">
<meta property="article:published_time" content="2022-12-31T12:01:30.000Z">
<meta property="article:modified_time" content="2023-08-19T07:30:50.083Z">
<meta property="article:author" content="Sinos">
<meta property="article:tag" content="frac">
<meta property="article:tag" content="gets">
<meta property="article:tag" content="exp">
<meta property="article:tag" content="decoder">
<meta property="article:tag" content="hat">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sinos_wei.gitee.io/img/cover/39.jpeg"><link rel="shortcut icon" href="/img/avatar.jpeg"><link rel="canonical" href="https://sinos_wei.gitee.io/2022/12/a8bdee1d82ac.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?aeddd8219bc5c1e1efdcf5d9cc3218e6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ai',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-19 15:30:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">207</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover/39.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Sinos_wei's blog"><span class="site-name">Sinos_wei's blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ai</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-31T12:01:30.000Z" title="Created 2022-12-31 20:01:30">2022-12-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-08-19T07:30:50.083Z" title="Updated 2023-08-19 15:30:50">2023-08-19</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ai"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
<span id="more"></span>
<h1 id="alternating-direction-method-of-multipliers-admm">Alternating direction method of multipliers (ADMM)</h1>
<p>交替方向乘子法（Alternating Direction Method of Multipliers，ADMM）是一种解决可分解凸优化问题的简单方法，尤其在解决大规模问题上卓有成效，利用ADMM算法可以将原问题的目标函数等价的分解成若干个可求解的子问题，然后并行求解每一个子问题，最后协调子问题的解得到原问题的全局解。ADMM 最早分别由 Glowinski &amp; Marrocco 及 Gabay &amp; Mercier 于 1975 年和 1976 年提出，并被 Boyd 等人于 2011 年重新综述并证明其适用于大规模分布式优化问题。由于 ADMM 的提出早于大规模分布式计算系统和大规模优化问题的出现，所以在 2011 年以前，这种方法并不广为人知。</p>
<h1 id="gnn-聚合和组合">gnn 聚合和组合</h1>
<p>https://towardsdatascience.com/what-can-you-do-with-gnns-5dbec638b525</p>
<figure>
<img src="https://miro.medium.com/max/1400/1*hj_tH5CCOhayQQt-YhGu5A.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>Any GNN can be represented as a layer containing two mathematical operators, <strong>aggregation function</strong> and <strong>combination function</strong>. This is best understood using the <strong>MPNN</strong> (message passing neural network) framework.</p>
<p><strong>aggregation</strong>: each message is normalized by sequare root of product of degrees of <strong>v</strong> and <strong>u</strong> 也就是把上一层的邻接点的特征向量聚合(归一化), 聚合方法可以使平均, 最大, 最小等 例如 节点 6 的节点1 聚合 a61=X1/(sqrt(7*2)), 其中 X1 是节点 1 的特征, 7 是节点 6 的度, 2 是节点 1 的度</p>
<p><strong>combinetion</strong>: 把当前节点的上一层的特征值和本层当前节点的聚合特征 组合</p>
<p>this can be easily achieved by <strong>D<sup>(-1/2)</sup>XAD<sup>(-1/2)</sup></strong> Usually, Adjacency matrix is added with <strong>I</strong> (identity matrix) to incorporate node’s own features.</p>
<p>每增加一层, 聚合的信息就多一跳(一层就是节点和他的邻居, 两层就再加上邻居的邻居)</p>
<h1 id="l1-l2-正则化">L1 L2 正则化</h1>
<figure>
<img src="https://picx.zhimg.com/80/v2-21c3e9c1972631e78330f27f1ac1ac8b_1440w.webp?source=1940ef5c" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>p=1 p=2 代表 L1, L2</p>
<p>因为机器学习中众所周知的过拟合问题，所以用正则化防止过拟合，成了机器学习中一个非常重要的技巧。</p>
<p>但数学上来讲，其实就是在损失函数中加个<strong>正则项（Regularization Term）</strong>，来防止参数拟合得过好。</p>
<h1 id="支持向量机"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49331510">支持向量机</a></h1>
<p>SVM是什么? 先来看看维基百科上对<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">SVM的定义</a>:</p>
<blockquote>
<p>支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。</p>
</blockquote>
<p>如果从未接触SVM的话，维基的这一大段解释肯定会让你一头雾水。简单点讲，SVM就是一种二类分类模型，他的基本模型是的定义在特征空间上的<strong>间隔最大</strong>的线性分类器，SVM的学习策略就是间隔最大化。</p>
<h1 id="决策树">决策树</h1>
<p>决策树是一种十分常用的分类方法，需要监管学习（有教师的Supervised Learning），监管学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。</p>
<h1 id="卷积">卷积</h1>
<p>https://www.zhihu.com/question/22298352</p>
<p>对卷积这个名词的理解：<strong>所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。</strong></p>
<figure>
<img src="https://picd.zhimg.com/80/v2-de38ad49f9a1c99dafcc5d0a7fcac2ef_1440w.webp?source=1940ef5c" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h1 id="神经网络">神经网络</h1>
<h2 id="简单神经网络架构">简单神经网络架构</h2>
<p>基本神经网络的相互连接的人工神经元分为三层：</p>
<ul>
<li><p>输入层</p>
<p>来自外部世界的信息通过输入层进入人工神经网络。输入节点对数据进行处理、分析或分类，然后将其继续传递到下一层。</p></li>
<li><p>隐藏层</p>
<p>隐藏层从输入层或其他隐藏层获取其输入。人工神经网络可以具有大量的隐藏层。每个隐藏层都会对来自上一层的输出进行分析和进一步处理，然后将其继续传递到下一层。</p></li>
<li><p>输出层</p>
<p>输出层提供人工神经网络对所有数据进行处理的最终结果。它可以包含单个或多个节点。例如，如果我们要解决一个二元（是/否）分类问题，则输出层包含一个输出节点，它将提供 1 或 0 的结果。但是，如果我们要解决一个多类分类问题，则输出层可能会由一个以上输出节点组成。</p></li>
</ul>
<h2 id="深度神经网络架构">深度神经网络架构</h2>
<p>深度神经网络又名深度学习网络，拥有多个隐藏层，包含数百万个链接在一起的人工神经元。名为权重的数字代表节点之间的连接。如果节点之间相互激励，则该权重为正值，如果节点之间相互压制，则该权重为负值。节点的权重值越高，对其他节点的影响力就越大。 从理论上讲，深度神经网络可将任何输入类型映射到任何输出类型。但与其他机器学习方法相比，它们也需要更多大量的训练。它们需要数百万个训练数据示例，而不像较简单的网络那样，可能只需数百或数千个训练数据示例。</p>
<h2 id="分类">分类</h2>
<h3 id="前馈神经网络">前馈神经网络</h3>
<p>前馈神经网络以从输入节点到输出节点的单向方式处理数据。一层中的每个节点均与下一层中的每个节点连接。前馈网络使用反馈流程随着时间推移改进预测。</p>
<h3 id="反向传播算法">反向传播算法</h3>
<p>人工神经网络使用校正反馈循环不断学习，以改进其预测分析。简而言之，您可以认为数据通过神经网络中的很多不同路径从输入节点流动到输出节点。只有一条路径是正确的，可将输入节点映射到正确的输出节点。为了找到这条路径，神经网络将使用反馈循环，其工作原理如下：</p>
<ol type="1">
<li>每个节点都会猜测该路径中的下一个节点。</li>
<li>它将检查猜测是否正确。节点将为引发更正确猜测的路径分配更高的权重值，而为引发不正确猜测的节点路径分配更低的权重值。</li>
<li>对于下一个数据点，节点将使用更高权重的路径进行新的预测，然后重复第 1 步。</li>
</ol>
<h3 id="卷积神经网络">卷积神经网络</h3>
<p>卷积神经网络中的隐藏层执行特定的数学函数（如汇总或筛选），称为卷积。它们对于图像分类非常有用，因为它们可从图像中提取对图像识别和分类有用的相关特征。这种新形式更易于处理，而不会丢失对做出良好预测至关重要的特征。每个隐藏层提取和处理不同的图像特征，如边缘、颜色和深度。</p>
<h1 id="损失函数">损失函数</h1>
<ul>
<li>L2 Loss</li>
</ul>
<p><span class="math display">\[
l(y, y&#39;) = 1/2(y-y&#39;)^{2}
\]</span></p>
<ul>
<li><p>L1 Loss</p>
<p><span class="math display">\[
l(y, y&#39;) = |y-y&#39;|
\]</span></p></li>
<li><p>Huber's Robust Loss</p>
<p><span class="math display">\[
l(y, y&#39;)=\left\{
\begin{gather*} 
|y-y&#39;| - 1/2 &amp;&amp; |y-y&#39;|&gt;1\\
1/2(y-y&#39;)^{2} &amp;&amp; else
\end{gather*} 
\right.
\]</span></p></li>
</ul>
<h1 id="激活函数">激活函数</h1>
<ul>
<li><p>sigmoid σ</p>
<p><span class="math display">\[
sigmoid(x) = \frac{1}{1+exp(-x)}
\]</span></p></li>
<li><p>tanh 解决了 sigmoid 函数不易收敛的问题</p>
<p><span class="math display">\[
tanh(x) = \frac{1-exp(-2x)}{1+exp(-2x)}
\]</span></p></li>
<li><p>ReLU rectified linear unit 解决了 tanh 梯度消失的问题, 有稀疏性</p>
<p><span class="math display">\[
ReLU(x) = max(x, 0)
\]</span></p></li>
</ul>
<h1 id="神经网络结构">神经网络结构</h1>
<h2 id="全连接层">全连接层</h2>
<p>全连接层是一种神经网络层，也叫做“密集层”。这种层在神经网络中的作用是将输入转换为输出。</p>
<p>在全连接层中，每个输入单元都与每个输出单元相连。这种连接方式使得全连接层能够学习高度非线性的模式。</p>
<p>全连接层通常用于深度学习模型中的分类和回归任务。</p>
<h2 id="卷积层">卷积层</h2>
<p>平移不变性, 局部性</p>
<p>卷积层是一个特殊的全连接层</p>
<p>卷积层是神经网络中的一种常见层，主要用于图像处理任务。卷积层对输入数据进行卷积运算，并生成输出。</p>
<p>卷积层的输入是一个二维或三维张量，输出也是一个二维或三维张量。卷积层的参数包括卷积核（也称为滤波器）和偏差。卷积核是一个小的二维张量，它在输入数据上滑动，对输入进行卷积运算。</p>
<p>卷积层的主要作用是提取输入数据中的特征。例如，在图像分类任务中，卷积层可以学习到图像中的边缘、角点等特征。卷积层还可以减少输入数据的维度，并使用池化层来进一步降低数据维度。</p>
<p>核 kernel</p>
<h2 id="池化层">池化层</h2>
<p>类似卷积层, 比如最大池化层就是在 kernel 中找最大值, 平均池化层</p>
<p>池化层是神经网络中的一种常见层，主要用于图像处理任务。池化层对输入数据进行池化操作，并生成输出。</p>
<p>池化层的输入是一个二维或三维张量，输出也是一个二维或三维张量。池化层没有可学习的参数，它的作用主要是对输入数据进行降维和降采样。</p>
<p>池化层通常与卷积层一起使用，用于减小输入数据的维度和提取输入数据的重要特征。池化层还可以防止过拟合，并加速模型的训练过程。, 一般一个卷积层一个池化层, 卷积层的输出作为池化层的输入</p>
<h2 id="归一化层">归一化层</h2>
<p>归一化层是一种神经网络层，它的作用是对输入数据进行归一化处理。归一化层通常用于图像处理任务中，可以帮助模型更好地收敛。</p>
<p>归一化层的输入是一个二维或三维张量，输出也是一个二维或三维张量。归一化层没有可学习的参数，它的作用是将输入数据的均值变为0，方差变为1。</p>
<p>例如，在使用归一化层对图像数据进行归一化处理时，可以将每个像素的值减去整个图像的均值，再除以整个图像的标准差。这样可以使得图像数据的值分布在0附近，有助于加速模型的训练。</p>
<p>归一化层通常与卷积层或全连接层一起使用，可以帮助模型更好地提取特征。</p>
<h1 id="softmax回归">softmax回归</h1>
<p><span class="math display">\[
\hat{y_{i}} =   \frac{exp(o_{i})} {\sum_{k}exp(o_{k})}
\]</span></p>
<p>输入为一维向量, 输出每个维度的概率</p>
<ol type="1">
<li>值变为非负</li>
<li>分母为 1</li>
<li>值为概率</li>
</ol>
<p>使用交叉熵衡量预测和标号的区别</p>
<h1 id="mlp多层感知机">MLP多层感知机</h1>
<h2 id="单层感知机">单层感知机</h2>
<p><span class="math display">\[
\begin{aligned}
o=\sigma(&lt;w,x&gt;+b)\\
\sigma = \left\{
\begin{gather*} 
1 &amp;&amp; x&gt;0\\
0 &amp;&amp; else
\end{gather*} 
\right.
\end{aligned}
\]</span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">initialize w = 0 and b = 0 </span><br><span class="line">repeat</span><br><span class="line">    if yi[&lt;w, xi&gt;+b] ≤ 0 \\分类错误</span><br><span class="line">    then</span><br><span class="line">      w&lt;-w+yi xi and b&lt;-b+yi; \\更新 w,x</span><br><span class="line">  end if </span><br><span class="line">until all classified correctly</span><br></pre></td></tr></table></figure>
<p>等价于批量为 1 的梯度下降</p>
<h2 id="多层感知机">多层感知机</h2>
<p>解决了 XOR 问题</p>
<p>隐藏层?</p>
<p>最典型的MLP包括包括三层：<strong>输入层、隐层(至少一个)和输出层，MLP神经网络不同层之间是全连接的</strong>（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。<strong>权重、偏置和激活函数</strong></p>
<h1 id="rnn">RNN</h1>
<p>对于序列模型的神经网络</p>
<p>循环神经网络</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">o1  o2  o3  o4</span><br><span class="line">|   |   |   |</span><br><span class="line"><span class="meta prompt_">h1-&gt;</span><span class="language-bash">h2-&gt;h3-&gt;h4</span></span><br><span class="line">    |   |   |</span><br><span class="line">    x1  x2  x3  x4</span><br></pre></td></tr></table></figure>
<p>相比 MLP, 多了对 h<sub>t-1</sub>的权重</p>
<h1 id="梯度下降gd">梯度下降GD</h1>
<p><span class="math display">\[
\begin{gather*}
\theta \gets \theta - \varepsilon g\\
\varepsilon 学习率\\
g 梯度
\end{gather*}
\]</span></p>
<h2 id="随机梯度下降-sgd">随机梯度下降: SGD</h2>
<p>每次随机抽取样本计算梯度</p>
<p>使用动量的随机梯度下降: 每次更新是给之前的梯度一个权重, 称之为动量</p>
<p>学习率 AdaGrad 算法</p>
<p><span class="math display">\[
\begin{gather*}
r \gets r + g^{2}\\
\theta \gets \theta - \frac{\varepsilon}{\sqrt{r} + \delta}g\\
\delta 为一个小量,稳定数值计算
\end{gather*}
\]</span></p>
<p>RMSProp 算法更新 r&lt;-ρr+(1-ρ)g<sup>2</sup></p>
<h2 id="adam-算法">Adam 算法</h2>
<blockquote>
<p>L 损失函数</p>
</blockquote>
<p><span class="math display">\[
\begin{gather*}
g=\frac{1}{m}\nabla_{\theta}\sum^{m}_{i=1}L(f(x_{i}, \theta), y_{i})\\
s\gets\rho_{1}s+(1-\rho_{1})g\\
r\gets\rho_{2}r+(1-\rho_{2})g^{2}\\
\hat{s}\gets\frac{s}{1-\rho^{t}_{1}}\\
\hat{r}\gets\frac{r}{1-\rho^{t}_{}}\\
\theta \gets \theta - \frac{\varepsilon \hat{s}}{\sqrt{\hat{r}} + \delta}\\
\end{gather*}
\]</span></p>
<h2 id="反向传播bp">反向传播bp</h2>
<p>反向传播算法就是神经网中加速计算参数梯度值的算法</p>
<h1 id="gnn">GNN</h1>
<p>https://www.jianshu.com/p/5d3eaa1f5e20</p>
<h1 id="dnn">DNN</h1>
<p>深度神经网络</p>
<p>有时候也叫多层感知机</p>
<h1 id="encoder-decoder-架构">encoder decoder 架构</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌──────┐┌───┐┌──────┐</span><br><span class="line">│never1││say││never2│</span><br><span class="line">└┬─────┘└┬──┘└┬─────┘</span><br><span class="line">┌▽───────▽────▽┐     </span><br><span class="line">│encoder       │     </span><br><span class="line">└┬───┬───┬─────┘     </span><br><span class="line">┌▽─┐┌▽─┐┌▽─┐         </span><br><span class="line">│v1││v2││v3│         </span><br><span class="line">└┬─┘└┬─┘└┬─┘         </span><br><span class="line">┌▽───▽───▽┐          </span><br><span class="line">│input    │          </span><br><span class="line">└┬────────┘          </span><br><span class="line">┌▽──────────┐        </span><br><span class="line">│decoder    │        </span><br><span class="line">└───────────┘        </span><br></pre></td></tr></table></figure>
<h1 id="attention-注意力机制">attention 注意力机制</h1>
<p>随意线索被称之为查询 (query)</p>
<p>每个输入是一个值(value) 和不随意线索 (key）的对</p>
<p>通过注意力池化层来有偏向性的选择选择某些输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌─────────┐┌───────────┐┌──────────┐ </span><br><span class="line">│x1       ││x2         ││x3        │ </span><br><span class="line">└┬───┬───┬┘└─┬────┬───┬┘└─┬───┬───┬┘ </span><br><span class="line">┌▽─┐┌▽─┐┌▽─┐┌▽──┐┌▽─┐┌▽─┐┌▽─┐┌▽─┐┌▽─┐</span><br><span class="line">│q1││v1││k1││q2 ││k2││v2││k3││v3││q3│</span><br><span class="line">└──┘└┬─┘└┬─┘└┬┬┬┘└┬─┘└┬─┘└┬─┘└┬─┘└──┘</span><br><span class="line">     │   │  ┌│││──│───│───┘   │      </span><br><span class="line">     │ ┌─│──││││──┘   │       │      </span><br><span class="line">   ┌─│─│─│──│┘││      │       │      </span><br><span class="line">┌──│─┘ │┌│──│─┘│   ┌──┘       │      </span><br><span class="line">│  │┌──││┘  │ ┌┘   │┌─────────┘      </span><br><span class="line">│┌─▽▽┐┌▽▽─┐┌▽─▽┐   ││                </span><br><span class="line">││a21││a22││a23│   ││                </span><br><span class="line">│└┬──┘└┬──┘└┬──┘   ││                </span><br><span class="line">│┌▽────▽────▽──┐   ││                </span><br><span class="line">││softmax      │   ││                </span><br><span class="line">│└┬─────┬─────┬┘   ││                </span><br><span class="line">│┌▽───┐┌▽───┐┌▽───┐││                </span><br><span class="line">││a21&#x27;││a23&#x27;││a22&#x27;│││                </span><br><span class="line">│└┬───┘└──┬─┘└┬───┘││                </span><br><span class="line">│ │      ┌│───│────│┘                </span><br><span class="line">│ │  ┌───││───│────┘                 </span><br><span class="line">└┐│  │┌──││───┘                      </span><br><span class="line">┌▽▽┐┌▽▽┐┌▽▽┐                         </span><br><span class="line">│*1││*2││*3│                         </span><br><span class="line">└┬─┘└┬─┘└┬─┘                         </span><br><span class="line">┌▽───▽───▽┐                          </span><br><span class="line">│    y    │                          </span><br><span class="line">└─────────┘                          </span><br></pre></td></tr></table></figure>
<h1 id="transformer">transformer</h1>
<p>使用encoder-decoder 架构</p>
<p>注意力机制+自循环</p>
<p>自循环体现在 decoder 将输出作为下一轮的输入, 同时在decoder 的 attention 中将未初始化向量的<span class="math inline">\(\alpha\)</span>置为<span class="math inline">\(-\infty\)</span> softmax后的<span class="math inline">\(\alpha&#39; = 0\)</span></p>
<h1 id="pyg">PyG</h1>
<h1 id="dgl">DGL</h1>
<h1 id="dygnn">DyGNN</h1>
<h1 id="sgcn">SGCN</h1>
<h1 id="线性代数">线性代数</h1>
<p>矩阵-&gt;每个列都是 基底, 每个矩阵代表了一种线性变换</p>
<p>复合变化: 从右往左</p>
<p>行列式的值: 线性变化后对应面积(体积)的变化</p>
<p>秩: 线性变换后的维数</p>
<p>点积: 投影的乘积</p>
<p>​ 列向量 点积 列向量 = 行向量 * 列向量</p>
<p>叉积: 向量组成的行列式的值(方向垂直)</p>
<p>线性方程组</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 2 3   x    1</span><br><span class="line">0 1 2   y  = 1</span><br><span class="line">2 3 4   z    1</span><br><span class="line"></span><br><span class="line">1 2  x</span><br><span class="line">0 1  y 这是一个从二维到三维的变换</span><br><span class="line">2 3 </span><br></pre></td></tr></table></figure>
<p><span class="math display">\[
λ_{uv}=\frac{||h_u - h_v||_1}{d}, #(1)
\]</span></p>
<p>systolic array</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://sinos_wei.gitee.io">Sinos</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://sinos_wei.gitee.io/2022/12/a8bdee1d82ac.html">https://sinos_wei.gitee.io/2022/12/a8bdee1d82ac.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/frac/">frac</a><a class="post-meta__tags" href="/tags/gets/">gets</a><a class="post-meta__tags" href="/tags/exp/">exp</a><a class="post-meta__tags" href="/tags/decoder/">decoder</a><a class="post-meta__tags" href="/tags/hat/">hat</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/39.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/804677221633.html" title="cpp20入门读书笔记"><img class="cover" src="/img/cover/4.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">cpp20入门读书笔记</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/331cfaa59458.html" title="tcpip详解读书笔记"><img class="cover" src="/img/cover/81.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">tcpip详解读书笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/05/f05961026253.html" title="数据结构"><img class="cover" src="/img/cover/43.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-09-06</div><div class="title">数据结构</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Sinos</div><div class="author-info__description">己所不欲, 勿施于人</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">207</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SinosGray"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/SinosGray" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1143474942@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="http://8.130.42.149:5230" target="_blank" title="memo"><i class="fab fa-heart"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#alternating-direction-method-of-multipliers-admm"><span class="toc-number">1.</span> <span class="toc-text">Alternating direction method of multipliers (ADMM)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gnn-%E8%81%9A%E5%90%88%E5%92%8C%E7%BB%84%E5%90%88"><span class="toc-number">2.</span> <span class="toc-text">gnn 聚合和组合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#l1-l2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">3.</span> <span class="toc-text">L1 L2 正则化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">5.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF"><span class="toc-number">6.</span> <span class="toc-text">卷积</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">7.1.</span> <span class="toc-text">简单神经网络架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">7.2.</span> <span class="toc-text">深度神经网络架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">7.3.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.3.1.</span> <span class="toc-text">前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">7.3.2.</span> <span class="toc-text">反向传播算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.3.3.</span> <span class="toc-text">卷积神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">9.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">10.</span> <span class="toc-text">神经网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-number">10.1.</span> <span class="toc-text">全连接层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">10.2.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">10.3.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="toc-number">10.4.</span> <span class="toc-text">归一化层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">11.</span> <span class="toc-text">softmax回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#mlp%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">12.</span> <span class="toc-text">MLP多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">12.1.</span> <span class="toc-text">单层感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">12.2.</span> <span class="toc-text">多层感知机</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn"><span class="toc-number">13.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dgd"><span class="toc-number">14.</span> <span class="toc-text">梯度下降GD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-sgd"><span class="toc-number">14.1.</span> <span class="toc-text">随机梯度下降: SGD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#adam-%E7%AE%97%E6%B3%95"><span class="toc-number">14.2.</span> <span class="toc-text">Adam 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbp"><span class="toc-number">14.3.</span> <span class="toc-text">反向传播bp</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gnn"><span class="toc-number">15.</span> <span class="toc-text">GNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dnn"><span class="toc-number">16.</span> <span class="toc-text">DNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#encoder-decoder-%E6%9E%B6%E6%9E%84"><span class="toc-number">17.</span> <span class="toc-text">encoder decoder 架构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">18.</span> <span class="toc-text">attention 注意力机制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer"><span class="toc-number">19.</span> <span class="toc-text">transformer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pyg"><span class="toc-number">20.</span> <span class="toc-text">PyG</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dgl"><span class="toc-number">21.</span> <span class="toc-text">DGL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dygnn"><span class="toc-number">22.</span> <span class="toc-text">DyGNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sgcn"><span class="toc-number">23.</span> <span class="toc-text">SGCN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">24.</span> <span class="toc-text">线性代数</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/f05961026253.html" title="数据结构"><img src="/img/cover/43.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构"/></a><div class="content"><a class="title" href="/2023/05/f05961026253.html" title="数据结构">数据结构</a><time datetime="2023-09-05T16:31:11.896Z" title="Updated 2023-09-06 00:31:11">2023-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/04/9ea3537bedb7.html" title="电影"><img src="/img/cover/8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="电影"/></a><div class="content"><a class="title" href="/2020/04/9ea3537bedb7.html" title="电影">电影</a><time datetime="2023-09-03T12:41:01.223Z" title="Updated 2023-09-03 20:41:01">2023-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/4eb2e1b2c3ff.html" title="system-design"><img src="/img/cover/68.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="system-design"/></a><div class="content"><a class="title" href="/2023/08/4eb2e1b2c3ff.html" title="system-design">system-design</a><time datetime="2023-08-31T03:34:07.014Z" title="Updated 2023-08-31 11:34:07">2023-08-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/dd0ba91b15ab.html" title="code-review"><img src="/img/cover/33.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="code-review"/></a><div class="content"><a class="title" href="/2023/07/dd0ba91b15ab.html" title="code-review">code-review</a><time datetime="2023-08-21T14:29:06.086Z" title="Updated 2023-08-21 22:29:06">2023-08-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/b615e50b2d89.html" title="clean-code"><img src="/img/cover/9.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="clean-code"/></a><div class="content"><a class="title" href="/2023/08/b615e50b2d89.html" title="clean-code">clean-code</a><time datetime="2023-08-21T14:19:50.585Z" title="Updated 2023-08-21 22:19:50">2023-08-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Sinos</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>